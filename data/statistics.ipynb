{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "def read_data(file_name):\n",
    "    items = []\n",
    "    for i in open(file_name,'r').readlines():\n",
    "        items.append(json.loads(i))\n",
    "    return pd.DataFrame(items)\n",
    "def save_data(df, o_name, suffix = 'json'):\n",
    "    df = df.astype(object)\n",
    "    with open(f\"{o_name}.{suffix}\",'w+') as t:\n",
    "        for i in tqdm(range(len(df))):\n",
    "            item = df.iloc[i,:].to_dict()\n",
    "            t.write(json.dumps(item)+'\\n')\n",
    "def save_dict(d, o_name):\n",
    "    with open(f\"{o_name}.json\",'w+') as o:\n",
    "        o.write(json.dumps(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80906, 480)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = 'train'\n",
    "train = read_data(f'./{tag}.json')\n",
    "len(train), len(train.problem_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4742, 565)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = 'valid'\n",
    "valid = read_data(f'./{tag}.json')\n",
    "len(valid), len(valid.problem_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3583, 120)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = 'test'\n",
    "test = read_data(f'./{tag}.json')\n",
    "len(test), len(test.problem_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def edit_distance(tokens1, tokens2):\n",
    "    m, n = len(tokens1), len(tokens2)\n",
    "    \n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if tokens1[i - 1] == tokens2[j - 1]:  \n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:  \n",
    "                dp[i][j] = min(dp[i - 1][j],     \n",
    "                               dp[i][j - 1],     \n",
    "                               dp[i - 1][j - 1]  \n",
    "                              ) + 1\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "tokens1 = [\"int\", \"main\", \"(\", \")\", \"{\", \"return\", \"0\", \";\", \"}\"]\n",
    "tokens2 = [\"int\", \"main\", \"(\", \"int\", \"argc\", \")\", \"{\", \"return\", \"1\", \";\", \"}\"]\n",
    "\n",
    "distance = edit_distance(tokens1, tokens2)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/share/liangqingyuan/Models/deepseek-coder-1.3b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3583/3583 [00:32<00:00, 111.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED:  0.12154382399194737\n",
      "ED:  19.8601730393525\n",
      "POS tokens:  133.35835891710857\n",
      "NEG tokens:  129.2416969020374\n",
      "PROBLEM tokens:  178.68333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "edit_distance_list = []\n",
    "rel_edit_dis = []\n",
    "pos_token_list = []\n",
    "neg_token_list = []\n",
    "problem_list = []\n",
    "unique_nl = []\n",
    "for _, item in tqdm(test.iterrows(), total=len(test)):\n",
    "    pos = item[\"pos\"]\n",
    "    neg = item[\"neg\"]\n",
    "    pos_tokens = tokenizer.tokenize(pos)\n",
    "    neg_tokens = tokenizer.tokenize(neg)\n",
    "    pos_token_list.append(len(pos_tokens))\n",
    "    neg_token_list.append(len(neg_tokens))\n",
    "    distance = edit_distance(pos_tokens, neg_tokens)\n",
    "    edit_distance_list.append(distance)\n",
    "    rel_edit_dis.append(distance / ((len(pos_tokens)+ len(neg_tokens)/2)))\n",
    "    if item['nl'] not in unique_nl:\n",
    "        unique_nl.append(item['nl'])\n",
    "        problem_list.append(len(tokenizer.tokenize(item['nl'])))\n",
    "print('RED: ', sum(rel_edit_dis)/len(rel_edit_dis))\n",
    "print('ED: ', sum(edit_distance_list)/len(edit_distance_list))\n",
    "print('POS tokens: ', sum(pos_token_list)/len(pos_token_list))\n",
    "print('NEG tokens: ', sum(neg_token_list)/len(neg_token_list))\n",
    "print('PROBLEM tokens: ', sum(problem_list)/len(problem_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80906/80906 [09:04<00:00, 148.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED:  0.11091876696767959\n",
      "ED:  15.51968951622871\n",
      "POS tokens:  112.82112575087139\n",
      "NEG tokens:  109.64739327120363\n",
      "PROBLEM tokens:  187.6375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "edit_distance_list = []\n",
    "rel_edit_dis = []\n",
    "pos_token_list = []\n",
    "neg_token_list = []\n",
    "problem_list = []\n",
    "unique_nl = []\n",
    "for _, item in tqdm(train.iterrows(), total=len(train)):\n",
    "    pos = item[\"pos\"]\n",
    "    neg = item[\"neg\"]\n",
    "    pos_tokens = tokenizer.tokenize(pos)\n",
    "    neg_tokens = tokenizer.tokenize(neg)\n",
    "    pos_token_list.append(len(pos_tokens))\n",
    "    neg_token_list.append(len(neg_tokens))\n",
    "    distance = edit_distance(pos_tokens, neg_tokens)\n",
    "    edit_distance_list.append(distance)\n",
    "    rel_edit_dis.append(distance / ((len(pos_tokens)+ len(neg_tokens)/2)))\n",
    "    if item['nl'] not in unique_nl:\n",
    "        unique_nl.append(item['nl'])\n",
    "        problem_list.append(len(tokenizer.tokenize(item['nl'])))\n",
    "print('RED: ', sum(rel_edit_dis)/len(rel_edit_dis))\n",
    "print('ED: ', sum(edit_distance_list)/len(edit_distance_list))\n",
    "print('POS tokens: ', sum(pos_token_list)/len(pos_token_list))\n",
    "print('NEG tokens: ', sum(neg_token_list)/len(neg_token_list))\n",
    "print('PROBLEM tokens: ', sum(problem_list)/len(problem_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4742/4742 [01:29<00:00, 53.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED:  0.09951244163072674\n",
      "ED:  22.783002952340784\n",
      "POS tokens:  202.30514550822437\n",
      "NEG tokens:  198.5183466891607\n",
      "PROBLEM tokens:  273.7362831858407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "edit_distance_list = []\n",
    "rel_edit_dis = []\n",
    "pos_token_list = []\n",
    "neg_token_list = []\n",
    "problem_list = []\n",
    "unique_nl = []\n",
    "for _, item in tqdm(valid.iterrows(), total=len(valid)):\n",
    "    pos = item[\"pos\"]\n",
    "    neg = item[\"neg\"]\n",
    "    pos_tokens = tokenizer.tokenize(pos)\n",
    "    neg_tokens = tokenizer.tokenize(neg)\n",
    "    pos_token_list.append(len(pos_tokens))\n",
    "    neg_token_list.append(len(neg_tokens))\n",
    "    distance = edit_distance(pos_tokens, neg_tokens)\n",
    "    edit_distance_list.append(distance)\n",
    "    rel_edit_dis.append(distance / ((len(pos_tokens)+ len(neg_tokens)/2)))\n",
    "    if item['nl'] not in unique_nl:\n",
    "        unique_nl.append(item['nl'])\n",
    "        problem_list.append(len(tokenizer.tokenize(item['nl'])))\n",
    "print('RED: ', sum(rel_edit_dis)/len(rel_edit_dis))\n",
    "print('ED: ', sum(edit_distance_list)/len(edit_distance_list))\n",
    "print('POS tokens: ', sum(pos_token_list)/len(pos_token_list))\n",
    "print('NEG tokens: ', sum(neg_token_list)/len(neg_token_list))\n",
    "print('PROBLEM tokens: ', sum(problem_list)/len(problem_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
